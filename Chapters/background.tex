\chapter{Background}
\section{\wasm}
\label{sec:wasm}
Here we present an overview of \wasm so reader's have some familiarity with it for when we present \name.
We do not cover the entirety of the \wasm language as presented in the 2017 paper, but rather selected important points about the syntax, semantics, and type system.
It is recommended that the reader first skim this section to understand the basics and then refer back while reading \autoref{chp:prechk} and \autoref{chp:metatheory}.

\subsection{\wasm Syntax}
\autoref{fig:wasmsyntaxtypes} shows the types of \wasm.
Primitive \wasm types and values, represented as $t$, include 32 and 64-bit floats and integers.
Packed types, $tp$, include 8, 16, and 32 bit integers, they are used in memory operations.
Function types, $tf$, map from one sequence of types to another, but they are just syntax, not function types in the traditional sense, as explained in \autoref{subsec:wasmtyping}.

\begin{figure}
\begin{math}
\begin{array}{rcl}
    t & ::= & \<ithreetwo> \mid \<isixfour> \mid \<fthreetwo> \mid \<fsixfour> \\
    tp & ::= & \<ieight> \mid \<isixteen> \mid \<ithreetwo> \\
    tf & ::= & t^{*} \rightarrow t^{*} \\
    tg & ::= & \text{mut}^{?}\; t
\end{array}
\end{math}
\caption{\wasm Types}
\label{fig:wasmsyntaxtypes}
\end{figure}

The \wasm syntax uses the Kleene star within its BNF to refer to possibly empty sequences.
Kleene stars are commonly used in regular expressions to denote any number of a certain pattern, for example, $Nat^{*}= 1\; 2\; 3$, or $Nat^{*} = 1\; 1$.
For example, instructions, represented by the metavariable $e$, are usually grouped into sequences $e^{*}$, which are possibly empty $\epsilon$.
$e_1$ and $e_2$ refer to different instructions (although the actual instruction they represent may be the same).
Similarly, $e_1^{*}$ and $e_2^{*}$ refer to different sequences of instructions (again, the sequences may be the same).

There is no requirement that a sequence of non-terminals, $e_1^{*}$, be made up of entirely the same pattern, unless it is explicitly written out as in $(t.const c)^{*}$.
For example, $e_1^{*}$ matches $(t.\<const> c_1)\; (t.\<const> c_2)\; (t.binop)$.
Further, we may separate out subsequences: from $(t.\<const> c)^{*}$ we may separate out $t^{*}$ and $c^{*}$ to refer to the sequences of types and constant values respectively.

We can use different annotations in place of the Kleene star to add additional information.
The Kleene star may be replaced with an exact value $n$ when we know that the sequence has length $n$ (\eg a sequence of 3 types be phrased as $t^{3}$).
We can also use a question mark to represent either an empty sequence ($\epsilon$), or a sequence with exactly one item (\eg, $v^{?}=v' \lor v^{?}=\epsilon$).

With this notation in mind, we can now look over the \wasm instructions in \autoref{fig:wasminstructions}.
As a convention, syntax written in a \tbbf{blue, bold font} denotes a keyword, while text written in $italics$ represents a metavariable.
Throughout the \wasm syntax there are many metavariables used to represent natural numbers: $n$ and $m$ are usually used for the table and memory sizes, $i$ and $j$ are often used as indexes (\eg to reference a local variable), $o$ and $align$ are used within memory operations (we replace $a$ with $align$ for clarity and since we use $a$ elsewhere), and lastly $c$ is used as a constant variable (which could also be a float).
Confusingly enough, $iN$ is used to annotate operations that support integers, and $fN$ is used to annotate operations that support floats.

Some instructions, such as $\<loop> tf\; e^{*} \<end>$ include a sequence of instructions $e^{*}$.
We refer to such instructions as block instructions, since they define control flow blocks for the instructions inside (not to be confused with the $\<block>$ insturction, which is a block instruction).
Similarly, we refer to $e^{*}$ as the body.

\begin{figure}
    \begin{math}
    \begin{array}{rcl}
        unop_{iN} & ::= & \<clz> \mid \<ctz> \mid \<popcnt> \\
        testop_{iN} & ::= & \<eqz> \\
        binop_{iN} & ::= & \<add> \mid \<sub> \mid \<shl> \mid \<or> \mid ... \\
        relop_{iN} & ::= & \<eq> \mid \<ne> \mid \<gt> \mid \<ge> \mid ... \\
        cvtop & ::= & \<convert> \mid \<reinterpret> \\
    \end{array}
    \end{math}

    \begin{math}
    \begin{array}{rcl}
        e & ::= & \<unreachable> \mid \<nop> \mid \<drop> \mid \<select> \mid \\
        && \<block>\; tf\; e^{*} \<end> \mid \<loop>\; tf\; e^{*} \<end> \mid \<if>\; tf\; e^{*} \<else> e^{*} \<end> \mid \\
        && \<br> i \mid \<brif> i \mid \<brtable> i^{+} \mid \<return> \mid \<call> i \mid \<callindirect> tf \mid \\
        && \<getlocal> i \mid \<setlocal> i \mid \<teelocal> i \mid \<getglobal> i \mid \\
        && \<setglobal> i \mid t.\<load> (tp\_sx)^{?}\; align\; o \mid t.\<store> tp^{?}\; align\; o \mid \\
        && \<currentmemory> \mid \<growmemory> \mid t.\<const> c \mid \\
        && t.unop_t \mid t.binop_t \mid t.testop_t \mid t.relop_t \mid t.cvtop\; t\_sx^{?} \\
    \end{array}
    \end{math}
    \caption{\wasm Instructions}
    \label{fig:wasminstructions}
\end{figure}

\wasm has modules that include functions ($f$), global variables ($glob$), an optional function table ($tab$), and an optional linear memory chunk ($mem$), as seen in \autoref{fig:wasmmodules}.
Functions, globals, the table, and memory can be imported, using $\<import> "name_1"\; "name_2"$, which imports $name_2$ from the file $name_1$.
Similarly, they can also be exported under any number of names using $\<export> "name"$.

Functions include a list of local variable declarations to use within the body (a sequence of instructions).
Additionally, function arguments are accessible as local variables within the body of functions.
Global variables may be mutable (although, exported global variables cannot be mutable, as we will see later), and are initialized via a sequence of instructions.
Function tables store references to functions that can be called using indirect function calls, they are used to more safely represent function pointers.
Indirect function calls must supply a function type that gets checked against the table function that ends up being called at run-time.
Linear memory is simply a very large continuous chunk of memory.
Memory load and store operations require run-time bounds checks to ensure that they operate within the chunk of memory.

\begin{figure}
    \begin{math}
    \begin{array}{rcl}
        im &::=& \<import> "name_1"\; "name_2" \\
        ex &::=& \<export> "name" \\
        f &::=& ex^{*}\; \<func> tf\; \<local>\; t^{*}\; e^{*} \mid ex^{*}\; \<func> tf\; im \\
        glob &::=& ex^{*}\; \<global> tg\; e^{*} \mid ex^{*}\; \<global> tg\; im \\
        tab &::=& ex^{*}\; \<table> n\; i^{*} \mid ex^{*}\; \<table> n\; im \\
        mem &::=& ex^{*}\; \<memory> n\; \mid ex^{*}\; \<memory> n\; im \\
        module &::=& \<module> f^{*}\; glob^{*}\; tab^{?}\; mem^{?}
    \end{array}
    \end{math}
    \caption{\wasm Module Definitions}
    \label{fig:wasmmodules}
\end{figure}

\subsection{\wasm Dynamic Semantics}
\label{subsec:wasmsemantics}
\wasm is a stack-based assembly language that uses reduction semantics.
Before we introduce the \wasm semantics, we first must introduce some administrative instructions that are used in the reduction relation.
\autoref{fig:wasmadmin} shows the new administrative instructions.

\begin{figure}
    \begin{math}
    \begin{array}{rcl}
        cl &::=& \{\text{inst } i, \text{func } f\} \\
        b &::=& 0x00, 0x01, ..., 0xff \\
        tabinst &::=& cl^{*} \\
        meminst &::=& b^{*} \\
        inst &::=& \{\text{func } cl^{*}, \text{glob } v^{*}, \text{tab } i^{?},\text{mem } i^{?}\} \\
        s &::=& \{\text{inst } inst^{*}, \text{tab } tabinst^{*}, \text{mem } meminst^{*}\} \\
        v &::=& t.\<const> c \\
        e &::=& ... \mid \<trap> \mid \<call> cl \mid \<label>_n\{ e^{*}\}\; e^{*}\<end> \mid \\
        && \<local>_n\{ i;v^{*}\}\; e^{*}\<end>\\
        L^0 &::=& v^{*}\; \square \; e^{*} \\
        L^{k+1} &::=& v^{*}\; \<label>_n\{ e^{*}\}\; L^{k}\<end> \; e^{*} \\
    \end{array}
    \end{math}
    \caption{\wasm Administrative instructions}
    \label{fig:wasmadmin}
\end{figure}

$cl$ represents a \wasm closures.
Closures include the module instance that the function is defined in, as well as the function definition (which cannot be an import) with any exports erased.
Intuitively, a closure represents a function closed under linking.
$b$ represents a byte.
The runtime store, $s$, includes runtime instances for every module ($inst^{*}$), as well as all of the tables ($tabinst^{*}$), and memory chunks ($meminst^{*}$).
In other words, $s$ includes full definitions for every module.
Module runtime instances, $inst$, refer to their table/memory (if they have one), by indexing into the list of runtime instances of tables/memory chunks.

A representation of values, $v$, is added as a metavariable to refer to constant instructions $t.\<const> c$.
A \emph{trap} ($\<trap>$) is the \wasm term for a run time error.
$\<call> cl$ is a function call on a closure.
As we will see, it is an intermediate step for performing both direct and indirect function calls.

We also have two types of block instructions introduced.
The first, label blocks, are used in handling control flow.
Specifically, they are used to hand branching.
All block instructions (\<block>, \<loop>, and \<if>) reduce to label blocks.
Local blocks can store instructions (inside the curly braces), and the annotation $n$ is equal to the expected number of inputs to those instructions.
This is explained more when we explain how branching works.

The second new block instruction is the local block.
Locals blocks are how closure calls get expanded.
They introduce an environment consisting of the module instance and local variables inside which their body is reduced \todo{Executed? Reduced? Can we use them interchangeably? I think this comes up in other places as well, something to watch out for}.

Finally, we introduce reduction contexts, $L^{k}$, where $k$ is the nesting depth.
Reduction contexts are based on label blocks, so $L^{k}$ essentially contains $k$ nested local blocks.

There are a few final notational digressions we must make before describing the reduction relation.
First, objects such as $s=\{\text{inst } inst^{*}, \text{tab } tabinst^{*}, \text{mem } meminst^{*}\}$ can be deferenced using their keywords (\eg inst).
For example, $s_\text{inst}=inst^{*}$ given the above definition of $s$.
Second, we can index into a sequence to get a specific element (\eg $inst^{*}(i)$ returns the $i$th $inst$ in $inst^{*}$).
Lastly, \wasm uses several shorthands to get information out of module instances in $s$: $s_\text{func}(i)(j)=s_\text{inst}(i)_\text{func}(j)$.
Essentially, this allows us to implicity deference the $i$th module instance to get the $j$th function inside of the instance.
This shorthand is used similarly for glob, tab, and mem.

\paragraph{The \wasm Reduction Relation} works on \emph{configurations}, that include the store $s$, local variables (represented as a sequence of values $v^{*}$), and the instruction sequence $e^{*}$.
The store, local variables, and module index are omitted when not used.
We present all the different flavors of reduction rules below.

\begin{mathpar}
    \boxed{s;v^{*};e^{*} \rightarrow s';v'^{*};e'^{*}}
\end{mathpar}

Some rules also include the current module index $i$, which is used when dereferencing the store to know which module instance to look at.
Instructions are reduced in place by decomposing the program using reduction contexts.
Intuitively, we pull out the next instruction to execute, reduce it, and plug the result back in.
The ``stack'' is just the sequence of values preceding the first reducible instruction.
When an instruction reduces to a value, that value becomes part of the stack and the next instruction is reduced.
This way of decomposing ensures that all of the instructions preceding an instruction getting reduced are values.

Binary and relation operations consume two values and either return the specified operation applied to those values, or trap if it is not defined (in the case of dividing by zero).
Test operators only consume one value, and do not trap, but are otherwise similar.
\<unreachable> causes a trap, \<nop> reduces to the empty sequence, and \<drop> consumes one value and reduces to the empty sequence.
We have shown the true case of select, which returns the second value consumes ($k+1$ is a common shorthand for a non-zero value).

\begin{mathpar}
    \begin{array}{rcl}
        (t.\<const> c_1)\; (t.\<const> c_2)\; t.binop &\hookrightarrow& t.\<const> c \\
        \text{if } c=binop(c_1,c_2) && \\ %% binop

        (t.\<const> c_1)\; (t.\<const> c_2)\; t.binop &\hookrightarrow& \<trap> \\ %% binop to trap
        otherwise && \\

        (t.\<const> c)\; t.testop &\hookrightarrow& \<ithreetwo>.testop_t(c) \\

        \<unreachable> &\hookrightarrow& \<trap> \\

        \<nop> &\hookrightarrow& \epsilon \\

        v\;\<drop> &\hookrightarrow& \epsilon \\

        v_1\;v_2\;(\<ithreetwo>.\<const> k+1)\;\<select> &\hookrightarrow& v_2 \\

        v_1\;v_2\;(\<ithreetwo>.\<const> k+1)\;\<select> &\hookrightarrow& v_1 \\
    \end{array}
\end{mathpar}

Blocks and loops reduce to labels.
Stored instructions are only added by reduced a \<loop>, in which case it stores the loop code so it can run the loop again.
We show the true case of an $\<if>$ block, which reduces to the first body inside of a block, the false case does the same but with the second body.
If the body of a label block is a \<trap> or a sequence of values then the \<trap>/values replace the block.
Since decomposition happens on label blocks, we have included the inductive reduction rule, which intuitively pulls instructions out of the context, reduces them outside the context, and then plugs them right back in.

\begin{mathpar}
    \inferrule[]{
        s;v^{*};e^{*} \hookrightarrow s';v'^{*};e'^{*}
    } {
        s;v^{*};L^k[e^{*}] \hookrightarrow s';v'^{*};L^k[e'^{*}]
    } \\

    \begin{array}{rcl}
        L^{0}[\<trap>] &\hookrightarrow& \<trap> \\

        v^n\;\<block>\; (t_1^{n}\rightarrow t_2^{m})\; e^{*} \<end> &\hookrightarrow& \<label>_m \{\} v^n\;e^{*} \<end> \\

        v^n\;\<loop>\; (t_1^{n}\rightarrow t_2^{m})\; e^{*} \<end> &
        \hookrightarrow&
        {\begin{stackTL}
            \<label>_n
            {\begin{stackTL}
                \{ \<loop>\; (t_1^{n}\rightarrow t_2^{m})\; e^{*}
                \\ \<end> \}
                \\ v^n\;e^{*}
            \end{stackTL}} \\
            \<end>
        \end{stackTL}} \\

        (\<ithreetwo>.\<const> 0)\; \<if>\; tf\; e_1^{*} \<else> e_2^{*} \<end> &\hookrightarrow& \<block>\; tf\; e_1 \<end> \\

        (\<ithreetwo>.\<const> k+1)\; \<if>\; tf\; e_1^{*} \<else> e_2^{*} \<end> &\hookrightarrow& \<block>\; tf\; e_1 \<end> \\

        \<label>_n\; \{ e_0^{*} \}\; v^{*} \<end> &\hookrightarrow& v^{*} \\

        \<label>_n\; \{ e_0^{*} \}\; \<trap> \<end> &\hookrightarrow& \<trap> \\
    \end{array}
\end{mathpar}

$\<brif>$ consumes a value and reduces to a branch if the value is non-zero, otherwise it reduces to the empty sequence.
$\<brtable>$ has a list of one or more numbers that it may branch to.
It consumes a $\<ithreetwo>\; k$ and branches based on the $k$th number or the last one if there is no $k$th number.
Branching from inside a label block essentially peels back the specified number of label blocks, and continues executing with the stored instructions $e_0^{*}$ of the containing label block.
\autoref{fig:branching} has several examples of branching in action.

\begin{mathpar}
    \begin{array}{rcl}
        \<label>_n\; \{ e_0^{*} \}\; L^j[v^{n}\; \<br> j] \<end> &\hookrightarrow& v^n\; e_0^{*} \\

        (\<ithreetwo>.\<const> 0)\; \<brif> j &\hookrightarrow& \epsilon \\

        (\<ithreetwo>.\<const> k+1)\; \<brif> j &\hookrightarrow& \<br> j \\

        (\<ithreetwo>.\<const> k)\; \<brtable> j_1^{k}\;j\;j_2^{*} &\hookrightarrow& \<br> j \\

        (\<ithreetwo>.\<const> k+n)\; \<brtable> j_1^{k} j &\hookrightarrow& \<br> j \\
    \end{array}
\end{mathpar}

\begin{figure}
\begin{math}
\begin{array}{l}
    {\begin{stackTL}
        \<label>_0
        {\begin{stackTL}
            \{\} \\
            \<label>_0 {\begin{stackTL}
                \{\} \\
                \<label>_0 \{\}\; \<br> 1 \<end>
            \end{stackTL}} \\
            \<nsend>
        \end{stackTL}} \\
        \<nsend>
    \end{stackTL}} \\
    \hookrightarrow \<label>_0 \{\}\; \<end> \\\\
    \<label>_0 \{ \<loop> \dots \<end> \}\; \<br> 0 \<end> \\
    \hookrightarrow \<loop> \dots \<end> \\\\
    \<label>_0
    \begin{stackTL}
        \{ \}\; \\
        \<label>_0 \{ \<loop> \dots \<end> \}\; \<br> 1 \<end>
    \end{stackTL} \\
    \<nsend> \\
    \hookrightarrow \epsilon
\end{array}
\end{math}
\caption{Branching Examples}
\label{fig:branching}
\end{figure}

Direct and indirect function calls are expanded in two steps.
First, the associated closure is fetched either from the current module instance (for direct calls) or from the table (for indirect calls, which traps if the type of the fetched closure doesn't match the expected type).
This step reduces a direct or indirect call to a $\<call> cl$.
Then, the closure body is placed into a local block with the arguments from the stack and locals declared by the function, which are zero-initialized, being used as the local variables.

\begin{mathpar}
    \begin{array}{rcl}
        s;\<call> j &\hookrightarrow_i& s_\text{func}(i,j) \\

        s;\<callindirect> j &\hookrightarrow_i& s_\text{tab}(i,j) \\
        \text{if } s_\text{tab}(i,j)_\text{code}=(\<func> tf\; \<local>\; t^{*}\; e^{*}) && \\

        s;\<callindirect> j &\hookrightarrow_i& \<trap> \\
        \text{otherwise} && \\

        v^{n}\;(\<call> cl) &\hookrightarrow_i&
        {\begin{stackTL}
            \<local>_m
            {\begin{stackTL}
                \{
                    {\begin{stackTL}
                        cl_\text{inst}; \\
                        v^{n}\;(t.\<const> 0)^{k}\}
                    \end{stackTL}} \\
                \<block>\;(\epsilon \rightarrow t_2^{m})\; e^{*} \\
                \<nsend>
            \end{stackTL}} \\
            \<nsend> \\
        \end{stackTL}} \\
    \end{array}
\end{mathpar}

The local block has the same module index, $i$, as the closure, so the body of the local block is reduced within the module that the closure is defined it, so it uses the global variables, table, and memory of the module instance referenced by $i$.
This is handled by the inductive reduction rule (which has much more of a small-step flavor).
Inside the local block is a label block, so at the top level of a function $\<br> 0$ is essentially equivalent to $\<return>$, except with an additional reduction step (in general, $\<return>$ can be thought of as $\<br> k$, where $k$ is the context depth).
\todo{Was that note confusing? Was that sentence confusing?}
Returning, somewhat similarly to branching, simply replaces the local block with the arguments to the return instruction, except that it skips over any label blocks.
If the body of a local block is a \<trap> or sequence of values, then that is what the local block reduces to, similar to branching.

\begin{mathpar}
    \inferrule[]{
            s;v^{*};e^{*} \hookrightarrow_i s';v'^{*};e'^{*}
        } {
            s;v_0^{*};\<local>_n \{ i;v^{*} \} e^{*} \<end> \hookrightarrow_j s';v_0^{*};\<local>_n \{i;v'^{*}\} e'^{*} \<end>
        } \\

    \begin{array}{rcl}
        \<local>_n \{ i;v_l^{*} \} v^{n} &\hookrightarrow& v^{n} \\
        \<local>_n \{ i;v_l^{*} \} \<trap> &\hookrightarrow& \<trap> \\
        \<local>_n \{ i;v_l^{*} \} L^{k}[v^n \<return>] &\hookrightarrow& v^{n} \\
    \end{array}
\end{mathpar}

Local variables are just a list of values at run time!
They are get/set by indexing into them, similar to, well, everything else in \wasm.
The same is true of global variables, except there is an extra step since they are stored in the current module instance inside the store $s$.

\begin{mathpar}
    \begin{array}{rcl}
        v_1^{j}\;v\;v_2^{};\<getlocal> j &\hookrightarrow& v \\

        v_1^{j}\;v\;v_2^{};v'\; (\<setlocal> j) &\hookrightarrow& v_1^{j}\;v'\;v_2^{};\epsilon \\

        v_1^{j}\;v\;v_2^{};v'\; (\<teelocal> j) &\hookrightarrow& v_1^{j}\;v'\;v_2^{};v' \\

        s;\<getglobal> j &\hookrightarrow_i& s_\text{glob}(i,j) \\

        s\; (\<setglobal> j) &\hookrightarrow_i& s';\epsilon \\

        \text{where} s' = s \text{ with } glob(i,j)=v' \\
    \end{array}
\end{mathpar}

Finally, there are the memory instructions.
One can load or store a value from or to memory, get the current memory size, or try to grow the memory.
There is a lot of minutia detail, but none of it is particularly important.
The key high level takeaway is that load and store will trap if the supplied index plus the static offset is out of bounds.
$|t|$ is used to represent the size of the type (\eg $|t| = 8$ bytes).
We omit two rules, one each for store and load, that include the ability to use packed types to load/store smaller values and to load signed/unsigned.

\begin{mathpar}
    \begin{array}{rcl}
        s;(\<ithreetwo>.\<const> k) &&\\
        (t.\<load> tp\_sx\; align\; o) &\hookrightarrow_i& s;(t.\<const> const_t(b^{*})) \\
        \text{if } s_\text{mem}(i,k+o,|t|)=b^{*} && \\

        &&\\

        s;(\<ithreetwo>.\<const> k) &&\\
        (t.\<load> tp\_sx\; align\; o) &\hookrightarrow_i& \<trap> \\
        \text{otherwise} && \\

        &&\\

        s;(\<ithreetwo>.\<const> k)\; (t.\<const> c) && \\
        (t.\<store> tp\_sx\; align\; o) &\hookrightarrow_i& s';\epsilon \\
        \text{if } s'=s &\text{ with }& \text{mem}(i,k+o,|t|)=bits_t(c) \\

        &&\\

        s;(\<ithreetwo>.\<const> k)\; (t.\<const> c) && \\
        (t.\<store> tp\_sx\; align\; o) &\hookrightarrow_i& \<trap> \\
        \text{otherwise} && \\

    \end{array}
\end{mathpar}

\begin{mathpar}
    \begin{array}{rcl}
        s;\<currentmemory> &\hookrightarrow_i& \<ithreetwo>.\<const> |s_\text{mem}(i,*) | / 64\text{Ki} \\

        &&\\

        s;(\<ithreetwo>.\<const> k)&&\\
        \<growmemory> &\hookrightarrow_i& s';\<ithreetwo>.\<const> | s'_\text{mem}(i,*) | / 64\text{Ki} \\
        \text{if } s'=s &\text{ with }& \text{mem}(i,*)=s_\text{mem}(i,*)(0)^{k*64\text{Ki}} \\

        &&\\

        s;(\<ithreetwo>.\<const> k)&&\\
        \<growmemory> &\hookrightarrow_i& \<ithreetwo>.\<const> (-1) \\
        \text{otherwise} && \\
    \end{array}
\end{mathpar}

\subsection{The \wasm Type System}
\label{subsec:wasmtyping}
Instructions in \wasm are typed under a module type context $C$.
$C$ keeps track of various module-level types: functions, globals, the table, memory, locals, the label stack (\ie the expected types for branching instructions), and the return stack (\ie the expected type of the return instruction).

$$ C::= \{ {\begin{stackTL}
    \text{func } tf^{*}, \text{ global } tg^{*}, \text{ table } n^{?}, \text{ memory } m^{?},
    \\ \text{local } t^{*}, \text{ label } (t^{*})^{*}, \text{ return } (t^{*})^{?} \}
\end{stackTL}} $$

\wasm is a stack-based language, so the type of an instruction in \wasm consists of a precondition and postcondition on the shape of the stack.
This can be viewed as though instructions \emph{consume} certain values from the stack and then \emph{produce} values to be pushed on the stack.
For example, a binary operation of some type $t$ consumes two values of the given type $t$ on the stack and produces a value of type $t$:

\[
    \inferrule{ }{C \vdash t.binop : t\; t \rightarrow t}
\]

The above example shows what a typical \wasm typing rule looks like.
The type associated with the instruction $t.binop$ is a \wasm function type, which is just the precondition (on the left of the $\rightarrow$) and postcondition (on the right of the $\rightarrow$) on the stack.
Most typing rules are for a single instruction and there are a few rules which can combine rules.
Thus, the main \wasm typing judgement is as follows:

$$\boxed{C \vdash e^{*} : tf}$$

We reproduce and explain a few selected typing rules from \wasm.
The rule for typing a block typechecks the body $e^{*}$ under the module type context with the postcondition $t_2^{m}$ appended to the label stack.
This is yet another common notational atrocity where $x,y$ means $x$ extended with $y$.
The branch rule accepts any precondition, extended with the $i$th postcondition on the label stack (counting backwards), and returns to any postcondition.
A branch will return the $n$ values before it, so it is ok if there are more values on the stack, as they will be discarded.
Execution does not proceed after branching, so the postcondition can be anything.
For unction calls we lookup the type of the function in the context.
Finally, setting a local variable consumes a value of the type that is given by looking up the type of the local in the context.

\begin{mathpar}
    \inferrule{ }{C \vdash t.binop : t\; t \rightarrow t}

    \inferrule{
        tf = t_1^{n} \rightarrow t_2^{m} \and
        C,\text{label}(t_2^{m})\vdash e^{*} : tf
    }{
        C \vdash \<block>\; tf\; e^{*} \<end> : tf
    }

    \inferrule{
        C,\text{label}(i) = t^{n}
    }{
        C \vdash \<br> i \<end> : t_1^{*}\;t^{n} \rightarrow t_2^{*}
    }

    \inferrule{
        C_\text{func}(i) = tf
    }{
        C \vdash \<call> i \<end> : tf
    }

    \inferrule{
        C_\text{local}(i) = t
    }{
        C \vdash \<setlocal> i : t \rightarrow \epsilon
    }
\end{mathpar}

The empty instruction sequence has an empty precondition and postcondition.
An instruction $e_2$ can be appended to a sequence of instructions $e_1^{*}$ if the precondition of $e_2$ is the same as the postcondition of $e_1^{*}$.
Then, the precondition of the full sequence $e_1^{*}\;e_2$ is the precondition of $e_1^{*}$ and the postcondition of $e_1^{*}\;e_2$ is the postcondition of $e_2$.

\begin{mathpar}
    \inferrule{ }{C \vdash \epsilon : \epsilon \rightarrow \epsilon}

    \inferrule{
        C \vdash e_1^{*} : t_1^{*} \rightarrow t_2^{*}
        C \vdash e_2 : t_2^{m} \rightarrow t_3^{*}
    }{
        C \vdash e_1^{*}\;e_2 : t_1^{*} \rightarrow t_3^{*}
    }
\end{mathpar}

\paragraph{Stack Polymorphism.}
To compose together the types of many instructions, it is necessary to carry around extra type information about the rest of the stack while type-checking instructions.
\emph{Stack polymorphism} allows extending the precondition and postcondition with the same data to thread unmodified parts of the stack through a list of instructions.
Intuitively, this allows you to ``forget'' the rest of the stack and focus only on the part being manipulated by the instruction being checked, after which point the ``forgotten'' part can be re-added.

For example, if the stack has the shape $\<isixfour>\; \<ithreetwo>\; \<ithreetwo>$, then stack polymorphism allows us to ignore $\<isixfour>$ and typecheck $\<ithreetwo>.binop$ with $\<ithreetwo>\;\<ithreetwo>$.
Then the stack would look like $\<ithreetwo>$, at which point we add $\<isixfour>$ back to the postcondtion to get $\<isixfour>\; \<ithreetwo>$ after executing $\<ithreetwo>.binop$.
